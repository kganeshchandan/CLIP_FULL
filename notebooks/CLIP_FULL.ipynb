{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['data'] = {\"qm9_broad_ir_path\":'/home2/kanakala.ganesh/ir_data/qm9_broad_ir.pkl',\n",
    "                  \"vocab_path\":'/home2/kanakala.ganesh/CLIP_PART_1/data/qm9_vocab.pkl',\n",
    "                  \"datafiles\" : {\n",
    "                        'train': '/home2/kanakala.ganesh/ir_data/raw_train.pickle',\n",
    "                        'test':  '/home2/kanakala.ganesh/ir_data/raw_test.pickle',\n",
    "                        'val':   '/home2/kanakala.ganesh/ir_data/raw_val.pickle'\n",
    "                        },\n",
    "                  \"normalization\" : \"unit\",\n",
    "                  \"shuffle\": True,\n",
    "                  \"batch_size\":64,\n",
    "                  \"seq_len\":70,\n",
    "                  \"splits\":[0.9, 0.1, 0.1],\n",
    "                  \"num_workers\":16\n",
    "                }\n",
    "\n",
    "config['molecule_encoder'] = {\n",
    "    'attention': 1,\n",
    "    'coords_weight' :1.0,\n",
    "    'device': \"cuda\",\n",
    "    'hidden_nf':256,\n",
    "    'in_edge_nf':0,\n",
    "    'in_node_nf':15,\n",
    "    'n_layers': 3,\n",
    "    'node_attr': 1,\n",
    "    'output_size':512\n",
    "}\n",
    "\n",
    "config['molecule_decoder'] = {\n",
    "    'in_size': 512,\n",
    "    'latent_size' : 512,\n",
    "    'hidden_size': 512,\n",
    "    'n_layers' : 7,\n",
    "    'n_heads' : 4\n",
    "}\n",
    "\n",
    "config['spectra_encoder'] = {\n",
    "    'd_ff': 1024,\n",
    "    'dropout': 0.0,\n",
    "    'dropout_emb': 0.1,\n",
    "    'h_dim': 128,\n",
    "    'max_time_steps': 1000,\n",
    "    'num_heads': 5,\n",
    "    'num_layers': 5,\n",
    "    'output_size': 512,\n",
    "    'patch_size': 7,\n",
    "    'use_clf_token': True,\n",
    "}\n",
    "\n",
    "config['train'] = {\n",
    "    'temperature' :0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a9fad5bece4a639d2a96486c49532b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3135667c1814d02b662fe87d0b031f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PrepareData import prepare_data\n",
    "dataloaders, max_charge, num_species = prepare_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import yaml\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import utils\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.nn import functional as F \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from qm9.data.utils import _get_species, initialize_datasets\n",
    "from qm9 import utils as qm9_utils\n",
    "from qm9.data.dataset import ProcessedDataset\n",
    "from qm9.data.prepare import prepare_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from qm9.data.utils import initialize_datasets\n",
    "from qm9.args import init_argparse\n",
    "from qm9.data.collate import collate_fn\n",
    "from models.vit import ViT\n",
    "from qm9.models import EGNN\n",
    "from qm9 import dataset\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "dtype = torch.float32\n",
    "\n",
    "from models.decoder import LatentToMol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_causal_mask(seq_len):\n",
    "    mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    mask.requires_grad = False\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vocab = pickle.load(open(config['data']['vocab_path'], 'rb'))\n",
    "        self.temperature = config['train']['temperature']\n",
    "        \n",
    "        self.Molecule_Encoder = EGNN(\n",
    "             in_node_nf = self.config['molecule_encoder']['in_node_nf'], \n",
    "             in_edge_nf = self.config['molecule_encoder']['in_edge_nf'], \n",
    "             hidden_nf = self.config['molecule_encoder']['hidden_nf'], \n",
    "             device = torch.device(self.config['molecule_encoder']['device']), \n",
    "             n_layers = self.config['molecule_encoder']['n_layers'], \n",
    "             coords_weight = self.config['molecule_encoder']['coords_weight'],\n",
    "             attention = self.config['molecule_encoder']['attention'], \n",
    "             node_attr = self.config['molecule_encoder']['node_attr'],\n",
    "            output_size = self.config['molecule_encoder']['output_size'],\n",
    "        )\n",
    "        \n",
    "        self.Spectra_Encoder = ViT(\n",
    "            patch_size = self.config['spectra_encoder']['patch_size'], \n",
    "            num_layers = self.config['spectra_encoder']['num_layers'], \n",
    "            h_dim = self.config['spectra_encoder']['h_dim'], \n",
    "            num_heads = self.config['spectra_encoder']['num_heads'], \n",
    "            output_size = self.config['spectra_encoder']['output_size'], \n",
    "            d_ff=self.config['spectra_encoder']['d_ff'], \n",
    "            max_time_steps=self.config['spectra_encoder']['max_time_steps'], \n",
    "            use_clf_token=self.config['spectra_encoder']['use_clf_token'],\n",
    "            dropout = self.config['spectra_encoder']['dropout'],\n",
    "            dropout_emb = self.config['spectra_encoder']['dropout_emb']   \n",
    "        )\n",
    "        \n",
    "        self.smiles_decoder = LatentToMol(\n",
    "            in_size=self.config['molecule_decoder']['latent_size'],\n",
    "            hidden_size=self.config['molecule_decoder']['hidden_size'], \n",
    "            n_layers=self.config['molecule_decoder']['n_layers'], \n",
    "            n_heads = self.config['molecule_decoder']['n_heads'],\n",
    "            seq_len=self.config['data']['seq_len'], \n",
    "            vocab = self.vocab)\n",
    "        \n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * self.temperature)\n",
    "        \n",
    "    \n",
    "    def forward_mol(self, data, max_charge, num_species):\n",
    "        batch_size = self.config['data']['batch_size']\n",
    "        batch_size, n_nodes, _ = data['positions'].size()\n",
    "        atom_positions = data['positions'].view(batch_size * n_nodes, -1).to(device, dtype)\n",
    "        atom_mask = data['atom_mask'].view(batch_size * n_nodes, -1).to(device, dtype)\n",
    "        edge_mask = data['edge_mask'].to(device, dtype)\n",
    "        one_hot = data['one_hot'].to(device, dtype)\n",
    "        charges = data['charges'].to(device, dtype)\n",
    "        \n",
    "        charge_scale = max_charge\n",
    "    \n",
    "        nodes = qm9_utils.preprocess_input(one_hot, \n",
    "                                    charges,\n",
    "                                    2,\n",
    "                                    charge_scale,\n",
    "                                    device)\n",
    "\n",
    "        nodes = nodes.view(batch_size * n_nodes, -1)\n",
    "        edges = qm9_utils.get_adj_matrix(n_nodes, batch_size, device)\n",
    "        \n",
    "        mol_features = self.Molecule_Encoder(h0=nodes, \n",
    "             x=atom_positions, \n",
    "             edges=edges, \n",
    "             edge_attr=None, \n",
    "             node_mask=atom_mask, \n",
    "             edge_mask=edge_mask,\n",
    "             n_nodes=n_nodes)\n",
    "        \n",
    "        mol_features = mol_features / mol_features.norm(dim=1, keepdim=True)\n",
    "        \n",
    "        return mol_features\n",
    "    \n",
    "    def forward_spec(self, data):\n",
    "        spectra = data['IR'].to(device, dtype)\n",
    "        spectra = torch.unsqueeze(spectra, 1)\n",
    "        spectra = torch.unsqueeze(spectra, 1)\n",
    "        \n",
    "        spectra_features = self.Spectra_Encoder(spectra)\n",
    "        spectra_features = spectra_features / spectra_features.norm(dim=1, keepdim=True)\n",
    "        \n",
    "        return spectra_features\n",
    "    \n",
    "    def forward_decoder(self, data, spec_latents):\n",
    "        smi = data['decoder_inp'].to(device)\n",
    "        tgt = data['decoder_tgt'].to(device)\n",
    "        tgt_padding_mask = data['tgt_padding_mask'].to(device)\n",
    "        tgt_mask = set_up_causal_mask(self.config['data']['seq_len']).to(device)\n",
    "        \n",
    "        pred = self.smiles_decoder(spec_latents,\n",
    "                                   smi,\n",
    "                                   tgt_mask,\n",
    "                                   tgt_padding_mask)\n",
    "        return pred\n",
    "        \n",
    "    def forward(self, data, max_charge, num_species):\n",
    "        logits_scale = self.logit_scale.exp()\n",
    "        \n",
    "        mol_latents = self.forward_mol(data, max_charge, num_species)\n",
    "        spec_latents = self.forward_spec(data)\n",
    "        \n",
    "        smile_preds = self.forward_decoder(data, spec_latents)\n",
    "        \n",
    "        return mol_latents, spec_latents, smile_preds, logits_scale, data['index'] \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (Molecule_Encoder): EGNN(\n",
       "    (embedding): Linear(in_features=15, out_features=256, bias=True)\n",
       "    (embedding_out): Linear(in_features=256, out_features=15, bias=True)\n",
       "    (gcl_0): E_GCL_mask(\n",
       "      (edge_mlp): Sequential(\n",
       "        (0): Linear(in_features=513, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): SiLU()\n",
       "      )\n",
       "      (node_mlp): Sequential(\n",
       "        (0): Linear(in_features=527, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (att_mlp): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (gcl_1): E_GCL_mask(\n",
       "      (edge_mlp): Sequential(\n",
       "        (0): Linear(in_features=513, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): SiLU()\n",
       "      )\n",
       "      (node_mlp): Sequential(\n",
       "        (0): Linear(in_features=527, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (att_mlp): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (gcl_2): E_GCL_mask(\n",
       "      (edge_mlp): Sequential(\n",
       "        (0): Linear(in_features=513, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): SiLU()\n",
       "      )\n",
       "      (node_mlp): Sequential(\n",
       "        (0): Linear(in_features=527, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (att_mlp): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (node_dec): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (graph_dec): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (Spectra_Encoder): ViT(\n",
       "    (proc): Sequential(\n",
       "      (0): Unfold(kernel_size=(1, 7), dilation=1, padding=0, stride=(1, 7))\n",
       "      (1): Transpose()\n",
       "      (2): Linear(in_features=7, out_features=128, bias=True)\n",
       "    )\n",
       "    (enc): ViTransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ViTransformerEncoderLayer(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mha): MultiHeadAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (1): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (2): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (3): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (4): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "            )\n",
       "            (linear): Linear(in_features=125, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ViTransformerEncoderLayer(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mha): MultiHeadAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (1): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (2): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (3): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (4): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "            )\n",
       "            (linear): Linear(in_features=125, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): ViTransformerEncoderLayer(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mha): MultiHeadAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (1): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (2): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (3): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (4): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "            )\n",
       "            (linear): Linear(in_features=125, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): ViTransformerEncoderLayer(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mha): MultiHeadAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (1): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (2): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (3): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (4): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "            )\n",
       "            (linear): Linear(in_features=125, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (4): ViTransformerEncoderLayer(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mha): MultiHeadAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (1): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (2): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (3): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "              (4): Head(\n",
       "                (q_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (k_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "                (v_lin): Linear(in_features=128, out_features=25, bias=False)\n",
       "              )\n",
       "            )\n",
       "            (linear): Linear(in_features=125, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pos_emb): Embedding(1000, 128)\n",
       "      (dropout_emb): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (mlp): Linear(in_features=128, out_features=512, bias=True)\n",
       "  )\n",
       "  (smiles_decoder): LatentToMol(\n",
       "    (embed): Embedding(25, 512, padding_idx=0)\n",
       "    (pe): PositionalEncodings(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (classifier): Linear(in_features=512, out_features=25, bias=True)\n",
       "    (trfmdecoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (latentencoder): LatentEncoder(\n",
       "      (featurizer): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): LeakyReLU(negative_slope=0.01)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Conv1d(1, 5, kernel_size=(3,), stride=(1,))\n",
       "        (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (4): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv1d(5, 512, kernel_size=(7,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (res_block): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CLIP(config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataloaders['train']):\n",
    "    data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_latents, spec_latents, smile_preds, logits_scale, ids = model(data, max_charge, num_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol_latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 70, 25])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smile_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
