{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader that has the data objects for EGNN, VIT, TrfmDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qm9 import dataset\n",
    "from qm9.models import EGNN\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import argparse\n",
    "from qm9 import utils as qm9_utils\n",
    "import utils\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from qm9.data.utils import _get_species, initialize_datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from qm9.data.dataset import ProcessedDataset\n",
    "from qm9.data.prepare import prepare_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from qm9.data.utils import initialize_datasets\n",
    "from qm9.args import init_argparse\n",
    "from qm9.data.collate import collate_fn\n",
    "import pickle\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummy_dataset(Dataset):\n",
    "    def __init__(self, data_list,  num_species, max_charge ):\n",
    "        self.data_list = data_list\n",
    "        self.max_charge = max_charge\n",
    "        self.num_species = num_species\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.data_list[item]\n",
    "    \n",
    "def combine_datasets(datafiles):\n",
    "    datasets = {}\n",
    "    for split, datafile in datafiles.items():\n",
    "        datasets[split] = pickle.load(open(datafile, 'rb'))\n",
    "\n",
    "    keys = [list(data.keys()) for data in datasets.values()]\n",
    "    assert all([key == keys[0] for key in keys]), 'Datasets must have same set of keys!'\n",
    "\n",
    "    all_species = _get_species(datasets, ignore_check=False)\n",
    "\n",
    "    num_pts = {'train': datasets['train']['index'].shape[0],\n",
    "            'test': datasets['test']['index'].shape[0], \n",
    "            'valid': datasets['val']['index'].shape[0]}\n",
    "    \n",
    "    datasets = {split: ProcessedDataset(data, num_pts=num_pts.get(\n",
    "        split, -1), included_species=all_species, subtract_thermo=False) for split, data in datasets.items()}\n",
    "    \n",
    "    ls = []\n",
    "    for i, data in enumerate(datasets['train']):\n",
    "        ls.append(data)    \n",
    "    for i, data in enumerate(datasets['test']):\n",
    "        ls.append(data)\n",
    "    for i, data in enumerate(datasets['val']):\n",
    "        ls.append(data)\n",
    "    full_dataset = dummy_dataset(ls, \n",
    "                                 datasets['train'].num_species, \n",
    "                                 datasets['train'].max_charge)\n",
    "    return full_dataset       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_data_map(path):\n",
    "    qm9_broad_ir = pickle.load(open(path, 'rb'))\n",
    "    smiles_id_map = {}\n",
    "    for id, row in tqdm(qm9_broad_ir.iterrows()):\n",
    "        smiles_id_map[int(row['ID'].split('_')[1])] = row['SMILES']\n",
    "        \n",
    "    id_ir_map = {}\n",
    "    for id, row in tqdm(qm9_broad_ir.iterrows()):\n",
    "        id_ir_map[int(row['ID'].split('_')[1])] = row['IR_Data']\n",
    "\n",
    "    del qm9_broad_ir\n",
    "    return smiles_id_map, id_ir_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the smiles-transformer build_vocab and build_dataset for generating\n",
    "# qm9_corpus.txt and qm9_vocab.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocab import WordVocab\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(id_ir_map, type=\"unit\"):\n",
    "    irs = []\n",
    "    for i in id_ir_map:\n",
    "        irs.append(id_ir_map[i])\n",
    "    irs = np.array(irs)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    irs_minmax = min_max_scaler.fit_transform(irs)\n",
    "    irs_standard = standard_scaler.fit_transform(irs)\n",
    "    irs_unitnorm = preprocessing.normalize(irs, norm='l2')\n",
    "\n",
    "    new_dict_minmax = {}\n",
    "    new_dict_standard = {}\n",
    "    new_dict_unitnorm = {}\n",
    "    # new_dict_original = {}\n",
    "    for i, j in enumerate(id_ir_map):\n",
    "        new_dict_minmax[j] = irs_minmax[i]\n",
    "        new_dict_standard[j] = irs_standard[i]\n",
    "        new_dict_unitnorm[j] = irs_unitnorm[i]\n",
    "    \n",
    "    if type == \"minmax\":\n",
    "        return new_dict_minmax\n",
    "    else:\n",
    "        return new_dict_unitnorm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Randomizer\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from enumerator import SmilesEnumerator\n",
    "from utils_decoder import split\n",
    "\n",
    "def set_up_causal_mask(seq_len):\n",
    "    mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    mask.requires_grad = False\n",
    "    return mask\n",
    "\n",
    "def set_up_padding_mask(max_len, no_of_words):\n",
    "    tgt_padding_mask = torch.ones([max_len, ])\n",
    "    tgt_padding_mask[:no_of_words] = 0.0\n",
    "    tgt_padding_mask = tgt_padding_mask.bool()\n",
    "    return tgt_padding_mask\n",
    "\n",
    "class ParentDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 clip_dataset, \n",
    "                 max_charge,\n",
    "                 num_species,\n",
    "                 smiles_id_map, \n",
    "                 vocab, \n",
    "                 ir_dict_norm, \n",
    "                 seq_len=70,\n",
    "                 transform=Randomizer()\n",
    "                 ):\n",
    "        self.clip_dataset = clip_dataset\n",
    "        self.smiles_id_map = smiles_id_map\n",
    "        self.ir_dict_norm = ir_dict_norm\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "        self.transform = transform\n",
    "        self.max_charge = max_charge\n",
    "        self.num_species = num_species\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.clip_dataset)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        data = self.clip_dataset[item]\n",
    "        \n",
    "        sm = self.smiles_id_map[data['index'].item()]\n",
    "        sm = self.transform(sm)\n",
    "        \n",
    "        ir = self.ir_dict_norm[data['index'].item()]\n",
    "        \n",
    "        content = [self.vocab.stoi.get(token, self.vocab.unk_index) for token in sm]\n",
    "        X = [self.vocab.sos_index] + content + [self.vocab.eos_index]\n",
    "        \n",
    "        inp_tokens = X[:-1].copy()\n",
    "        tgt_tokens = X[1:].copy()\n",
    "        \n",
    "        sample_size = len(inp_tokens)\n",
    "        \n",
    "        tgt_padding_mask = set_up_padding_mask(self.seq_len, sample_size)\n",
    "        \n",
    "        padding = [self.vocab.pad_index]*(self.seq_len - sample_size)\n",
    "        X.extend(padding)\n",
    "        inp_tokens.extend(padding)\n",
    "        tgt_tokens.extend(padding)\n",
    "        \n",
    "        # return {\n",
    "        #     \"index\": data['index'],\n",
    "        #     \"decoder_inp\":torch.tensor(inp_tokens),\n",
    "        #     \"decoder_tgt\":torch.tensor(tgt_tokens),\n",
    "        #     \"IR\":torch.tensor(ir),\n",
    "        #     \"tgt_padding_mask\":tgt_padding_mask,\n",
    "        #     \"num_atoms\": data['num_atoms'],\n",
    "        #     \"charges\":data[\"charges\"],\n",
    "        #     \"positions\" : data['positions'],\n",
    "        #     \"one_hot\" : data['one_hot']\n",
    "        # }\n",
    "        return (data['index'], torch.tensor(inp_tokens), torch.tensor(tgt_tokens), torch.tensor(ir), tgt_padding_mask, data['num_atoms'],data[\"charges\"],data['positions'],data['one_hot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataloaders( dataset, sizes = [0.9, 0.1, 0.1], batch_size=128, num_workers=16, shuffle=True):\n",
    "    \n",
    "    train_size = int(len(dataset)*sizes[0])\n",
    "    test_size = int(len(dataset)*(sizes[1]))\n",
    "    val_size = len(dataset) - train_size - test_size\n",
    "    train , test, val = torch.utils.data.random_split(dataset, [train_size, test_size, val_size])\n",
    "    \n",
    "    datasets = {'train':train, \n",
    "                'test': test,\n",
    "                'val':val}\n",
    "    \n",
    "    dataloaders = {split: DataLoader(dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                 shuffle=shuffle ,\n",
    "                                 num_workers=num_workers,\n",
    "                                 collate_fn=collate_fn)\n",
    "                                 for split, dataset in datasets.items()}\n",
    "    return dataloaders     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(config):\n",
    "    vocab = WordVocab.load_vocab(config['data']['vocab_path'])\n",
    "\n",
    "    PAD = vocab.pad_index # 0 \n",
    "    UNK = vocab.unk_index # 1\n",
    "    EOS = vocab.eos_index # 2\n",
    "    SOS = vocab.sos_index # 3\n",
    "    MASK = vocab.mask_index # 5\n",
    "\n",
    "    smiles_id_map, ir_id_map = id_data_map(config['data']['qm9_broad_ir_path'])\n",
    "    new_dict_norm = normalize_data(id_ir_map=ir_id_map, type=config['data']['normalization'])\n",
    "    full_dataset = combine_datasets(datafiles=config['data']['datafiles'])\n",
    "    final_dataset = ParentDataset(clip_dataset=full_dataset,\n",
    "                                  max_charge=full_dataset.max_charge,\n",
    "                                  num_species=full_dataset.num_species,\n",
    "                                  smiles_id_map=smiles_id_map, \n",
    "                                  ir_dict_norm=new_dict_norm, \n",
    "                                  seq_len=config['data']['seq_len'],\n",
    "                                  vocab=vocab \n",
    "                                  )\n",
    "    dataloaders = CreateDataloaders(final_dataset,\n",
    "                                    sizes=config['data']['splits'], \n",
    "                                    batch_size=config['data']['batch_size'],\n",
    "                                    num_workers=config['data']['num_workers'],\n",
    "                                    shuffle=config['data']['shuffle']\n",
    "                                    )\n",
    "    \n",
    "    return dataloaders, full_dataset.max_charge, full_dataset.num_species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "datafiles = {\n",
    "    'train': '/home2/kanakala.ganesh/ir_data/raw_train.pickle',\n",
    "    'test':  '/home2/kanakala.ganesh/ir_data/raw_test.pickle',\n",
    "    'val':   '/home2/kanakala.ganesh/ir_data/raw_val.pickle'\n",
    "}\n",
    "config['data'] = {\"qm9_broad_ir_path\":'/home2/kanakala.ganesh/ir_data/qm9_broad_ir.pkl',\n",
    "                  \"vocab_path\":'/home2/kanakala.ganesh/CLIP_PART_1/data/qm9_vocab.pkl',\n",
    "                  \"datafiles\" : datafiles,\n",
    "                  \"normalization\" : \"unit\",\n",
    "                  \"shuffle\": True,\n",
    "                  \"batch_size\":128,\n",
    "                  \"seq_len\":70,\n",
    "                  \"splits\":[0.9, 0.1, 0.1],\n",
    "                  \"num_workers\":16\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders = prepare_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a154b901fa5d48cb8724913226ba9ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1dc7b06eca454bb731b0a3a6abd46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = WordVocab.load_vocab(config['data']['vocab_path'])\n",
    "smiles_id_map, ir_id_map = id_data_map(config['data']['qm9_broad_ir_path'])\n",
    "new_dict_norm = normalize_data(id_ir_map=ir_id_map, type=config['data']['normalization'])\n",
    "full_dataset = combine_datasets(datafiles=config['data']['datafiles'])\n",
    "final_dataset = ParentDataset(clip_dataset=full_dataset,\n",
    "                                max_charge=full_dataset.max_charge,\n",
    "                                num_species=full_dataset.num_species,\n",
    "                                smiles_id_map=smiles_id_map, \n",
    "                                ir_dict_norm=new_dict_norm, \n",
    "                                seq_len=config['data']['seq_len'],\n",
    "                                vocab=vocab \n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = CreateDataloaders(final_dataset,\n",
    "                                sizes=config['data']['splits'], \n",
    "                                batch_size=config['data']['batch_size'],\n",
    "                                num_workers=config['data']['num_workers'],\n",
    "                                shuffle=config['data']['shuffle']\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/home2/kanakala.ganesh/CLIP_PART_1/qm9/data/collate.py\", line 74, in collate_fn\n    batch = {prop: batch_stack([mol[prop] for mol in batch]) for prop in batch[0].keys()}\nAttributeError: 'tuple' object has no attribute 'keys'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m dataloaders[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m      2\u001b[0m     data\n\u001b[1;32m      3\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1360\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/home2/kanakala.ganesh/CLIP_PART_1/qm9/data/collate.py\", line 74, in collate_fn\n    batch = {prop: batch_stack([mol[prop] for mol in batch]) for prop in batch[0].keys()}\nAttributeError: 'tuple' object has no attribute 'keys'\n"
     ]
    }
   ],
   "source": [
    "for data in dataloaders['train']:\n",
    "    data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
