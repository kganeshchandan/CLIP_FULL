{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader that has the data objects for EGNN, VIT, TrfmDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qm9 import dataset\n",
    "from qm9.models import EGNN\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import argparse\n",
    "from qm9 import utils as qm9_utils\n",
    "import utils\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from qm9.data.utils import _get_species, initialize_datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from qm9.data.dataset import ProcessedDataset\n",
    "from qm9.data.prepare import prepare_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from qm9.data.utils import initialize_datasets\n",
    "from qm9.args import init_argparse\n",
    "from qm9.data.collate import collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafiles = {\n",
    "    'train': '/home2/kanakala.ganesh/ir_data/norm_unit_train.pickle',\n",
    "    'test':  '/home2/kanakala.ganesh/ir_data/norm_unit_test.pickle',\n",
    "    'val':   '/home2/kanakala.ganesh/ir_data/norm_unit_val.pickle'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for split, datafile in datafiles.items():\n",
    "    datasets[split] = pickle.load(open(datafile, 'rb'))\n",
    "\n",
    "keys = [list(data.keys()) for data in datasets.values()]\n",
    "assert all([key == keys[0] for key in keys]), 'Datasets must have same set of keys!'\n",
    "\n",
    "all_species = _get_species(datasets, ignore_check=False)\n",
    "\n",
    "num_pts = {'train': datasets['train']['index'].shape[0],\n",
    "           'test': datasets['test']['index'].shape[0], \n",
    "           'valid': datasets['val']['index'].shape[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {split: ProcessedDataset(data, num_pts=num_pts.get(\n",
    "        split, -1), included_species=all_species, subtract_thermo=False) for split, data in datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121944"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets['train']) + len(datasets['test']) + len(datasets['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['num_atoms', 'charges', 'positions', 'index', 'A', 'B', 'C', 'mu', 'alpha', 'homo', 'lumo', 'gap', 'r2', 'zpve', 'U0', 'U', 'H', 'G', 'Cv', 'omega1', 'zpve_thermo', 'U0_thermo', 'U_thermo', 'H_thermo', 'G_thermo', 'Cv_thermo', 'IR', 'one_hot'])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(datasets['train']):\n",
    "    data\n",
    "    break\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummy_dataset(Dataset):\n",
    "    def __init__(self, data_list,  num_species, max_charge ):\n",
    "        self.data_list = data_list\n",
    "        self.max_charge = max_charge\n",
    "        self.num_species = num_species\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.data_list[item]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for i, data in enumerate(datasets['train']):\n",
    "    ls.append(data)\n",
    "    \n",
    "for i, data in enumerate(datasets['test']):\n",
    "    ls.append(data)\n",
    "\n",
    "for i, data in enumerate(datasets['val']):\n",
    "    ls.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = dummy_dataset(ls, datasets['train'].num_species, datasets['train'].max_charge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , data in enumerate(full_dataset):\n",
    "    data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home2/kanakala.ganesh/ir_data/full_egnn_vit.pickle', 'wb') as f:\n",
    "#     pickle.dump(full_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae9fd71d61c44a79fe16122a6da8c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac1f7a9e72c42f0af8eb2e9487d9643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qm9_broad_ir = pickle.load(open('/home2/kanakala.ganesh/ir_data/qm9_broad_ir.pkl', 'rb'))\n",
    "smiles_id_map = {}\n",
    "for id, row in tqdm(qm9_broad_ir.iterrows()):\n",
    "    smiles_id_map[int(row['ID'].split('_')[1])] = row['SMILES']\n",
    "    \n",
    "id_ir_map = {}\n",
    "for id, row in tqdm(qm9_broad_ir.iterrows()):\n",
    "    id_ir_map[int(row['ID'].split('_')[1])] = row['IR_Data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the smiles-transformer build_vocab and build_dataset for generating\n",
    "# qm9_corpus.txt and qm9_vocab.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocab import WordVocab\n",
    "import argparse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = WordVocab.load_vocab('data/qm9_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = pd.read_csv(\"data/qm9_corpus.txt\", header=None).values\n",
    "smiles_strings = [smi[0].replace(\" \", \"\") for smi in smiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(x) for x in smiles_strings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "irs = []\n",
    "for i in id_ir_map:\n",
    "    irs.append(id_ir_map[i])\n",
    "irs = np.array(irs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "irs_minmax = min_max_scaler.fit_transform(irs)\n",
    "irs_standard = standard_scaler.fit_transform(irs)\n",
    "irs_unitnorm = preprocessing.normalize(irs, norm='l2')\n",
    "\n",
    "new_dict_minmax = {}\n",
    "new_dict_standard = {}\n",
    "new_dict_unitnorm = {}\n",
    "# new_dict_original = {}\n",
    "for i, j in enumerate(id_ir_map):\n",
    "    new_dict_minmax[j] = irs_minmax[i]\n",
    "    new_dict_standard[j] = irs_standard[i]\n",
    "    new_dict_unitnorm[j] = irs_unitnorm[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "UNK = 1\n",
    "EOS = 2\n",
    "SOS = 3\n",
    "MASK = 4\n",
    "\n",
    "\n",
    "\n",
    "from dataset import Randomizer\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from enumerator import SmilesEnumerator\n",
    "from utils_decoder import split\n",
    "\n",
    "def set_up_causal_mask(seq_len):\n",
    "    mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    mask.requires_grad = False\n",
    "    return mask\n",
    "\n",
    "def set_up_padding_mask(max_len, no_of_words):\n",
    "    tgt_padding_mask = torch.ones([max_len, ])\n",
    "    tgt_padding_mask[:no_of_words] = 0.0\n",
    "    tgt_padding_mask = tgt_padding_mask.bool()\n",
    "    return tgt_padding_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['num_atoms', 'charges', 'positions', 'index', 'A', 'B', 'C', 'mu', 'alpha', 'homo', 'lumo', 'gap', 'r2', 'zpve', 'U0', 'U', 'H', 'G', 'Cv', 'omega1', 'zpve_thermo', 'U0_thermo', 'U_thermo', 'H_thermo', 'G_thermo', 'Cv_thermo', 'IR', 'one_hot'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParentDataset(Dataset):\n",
    "    def __init__(self, clip_dataset, smiles_id_map, vocab, ir_dict_norm, seq_len=70, transform=Randomizer()):\n",
    "        self.clip_dataset = clip_dataset\n",
    "        self.smiles_id_map = smiles_id_map\n",
    "        self.ir_dict_norm = ir_dict_norm\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.clip_dataset)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        data = self.clip_dataset[item]\n",
    "        \n",
    "        sm = self.smiles_id_map[data['index'].item()]\n",
    "        sm = self.transform(sm)\n",
    "        \n",
    "        ir = self.ir_dict_norm[data['index'].item()]\n",
    "        \n",
    "        content = [self.vocab.stoi.get(token, self.vocab.unk_index) for token in sm]\n",
    "        X = [self.vocab.sos_index] + content + [self.vocab.eos_index]\n",
    "        \n",
    "        inp_tokens = X[:-1].copy()\n",
    "        tgt_tokens = X[1:].copy()\n",
    "        \n",
    "        sample_size = len(inp_tokens)\n",
    "        \n",
    "        tgt_padding_mask = set_up_padding_mask(self.seq_len, sample_size)\n",
    "        \n",
    "        padding = [self.vocab.pad_index]*(self.seq_len - sample_size)\n",
    "        X.extend(padding)\n",
    "        inp_tokens.extend(padding)\n",
    "        tgt_tokens.extend(padding)\n",
    "        \n",
    "        return {\n",
    "            \"index\": data['index'],\n",
    "            \"decoder_inp\":torch.tensor(inp_tokens),\n",
    "            \"decoder_tgt\":torch.tensor(tgt_tokens),\n",
    "            \"IR\":torch.tensor(ir),\n",
    "            \"tgt_padding_mask\":tgt_padding_mask,\n",
    "            \"num_atoms\": data['num_atoms'],\n",
    "            \"charges\":data[\"charges\"],\n",
    "            \"positions\" : data['positions']\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = ParentDataset(clip_dataset=full_dataset,smiles_id_map=smiles_id_map, ir_dict_norm=new_dict_unitnorm, seq_len=70,vocab=vocab )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['index', 'decoder_inp', 'decoder_tgt', 'IR', 'tgt_padding_mask', 'num_atoms', 'charges', 'positions'])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(final_dataset):\n",
    "    data\n",
    "    break\n",
    "    \n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train , test, val = torch.utils.data.random_split(final_dataset, [100000,10000, len(final_dataset)- 110000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataloaders(dataset, sizes = [0.9, 0.1, 0.1]):\n",
    "    \n",
    "    \n",
    "    train , test, val = torch.utils.data.random_split(final_dataset, \n",
    "                                                     )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
