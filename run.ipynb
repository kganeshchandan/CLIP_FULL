{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc56899-351a-4b03-8e8d-a8e546c2e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['data'] = {\"qm9_broad_ir_path\":'/home2/kanakala.ganesh/ir_data/qm9_broad_ir.pkl',\n",
    "                  \"vocab_path\":'/home2/kanakala.ganesh/CLIP_PART_1/data/qm9_vocab.pkl',\n",
    "                  \"datafiles\" : {\n",
    "                        'train': '/home2/kanakala.ganesh/ir_data/raw_train.pickle',\n",
    "                        'test':  '/home2/kanakala.ganesh/ir_data/raw_test.pickle',\n",
    "                        'val':   '/home2/kanakala.ganesh/ir_data/raw_val.pickle'\n",
    "                        },\n",
    "                  \"normalization\" : \"minmax\",\n",
    "                  \"shuffle\": True,\n",
    "                  \"batch_size\":100,\n",
    "                  \"seq_len\":70,\n",
    "                  \"splits\":[0.8, 0.1, 0.1],\n",
    "                  \"num_workers\":20\n",
    "                }\n",
    "\n",
    "config['molecule_encoder'] = {\n",
    "    'attention': 1,\n",
    "    'coords_weight' :1.0,\n",
    "    'device': \"cuda\",\n",
    "    'hidden_nf':256,\n",
    "    'in_edge_nf':0,\n",
    "    'in_node_nf':15,\n",
    "    'n_layers': 3,\n",
    "    'node_attr': 1,\n",
    "    'output_size':512\n",
    "}\n",
    "\n",
    "config['molecule_decoder'] = {\n",
    "    'in_size': 512,\n",
    "    'latent_size' : 512,\n",
    "    'hidden_size': 512,\n",
    "    'n_layers' : 5,\n",
    "    'n_heads' : 4\n",
    "}\n",
    "\n",
    "config['spectra_encoder'] = {\n",
    "    'd_ff': 512,\n",
    "    'dropout': 0.0,\n",
    "    'dropout_emb': 0.1,\n",
    "    'h_dim': 512,\n",
    "    'max_time_steps': 500,\n",
    "    'num_heads': 7,\n",
    "    'num_layers': 5,\n",
    "    'output_size': 512,\n",
    "    'patch_size': 7,\n",
    "    'use_clf_token': True,\n",
    "}\n",
    "\n",
    "config['train'] = {\n",
    "    'lr':0.0001,\n",
    "    'temperature' :0.1,\n",
    "    'checkpoint_dir': \"checkpoints/temp\",\n",
    "    'device':\"cuda\",\n",
    "    'num_epochs':100,\n",
    "    'threshold': 0.99,\n",
    "    'weight_decay': 1.0e-06\n",
    "}\n",
    "\n",
    "config['wandb'] = {\n",
    "    \"dir\": \"/scratch/kanakala.ganesh/\",\n",
    "    \"job_type\": \"sample\",\n",
    "    \"project_name\": \"CLIP_Full_testing\",\n",
    "    \"run_name\": \"RUN_testing\"\n",
    "}\n",
    "config['data']['max_charge'] = None\n",
    "config['data']['num_species'] = None\n",
    "\n",
    "config['train']['logs'] = {\n",
    "            'train_total_loss':[],\n",
    "            'train_clip_loss':[],\n",
    "            'train_recon_loss':[],\n",
    "            \n",
    "            'val_total_loss':[],\n",
    "            'val_clip_loss':[],\n",
    "            'val_recon_loss':[],\n",
    "            \n",
    "            'test_total_loss':[],\n",
    "            'test_clip_loss':[],\n",
    "            'test_recon_loss':[],\n",
    "            \n",
    "            'best_epoch': -1,\n",
    "            'best_clip_epoch': -1,\n",
    "            'best_recon_epoch':-1,\n",
    "            \n",
    "            'best_total_loss':1000,\n",
    "            'best_clip_loss':1000,\n",
    "            'best_recon_loss':1000\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf911f4f-470c-4fc8-8138-3e0de2b59731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa2e409-a748-4690-b35a-3e5a16dd21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrepareData import prepare_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd37e8f-52ca-41b2-a7f6-c544ce8e9fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.nn import functional as F\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "import seaborn as sns\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0e9f85-4b94-49ab-b0a9-9d43fec779e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac6cc59-3ec2-4344-a820-e9c2024ddd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from architecture import CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba1c24f-89d9-4d2b-bdc3-7ab06f332176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(dataloaders['train']):\n",
    "#     data\n",
    "#     print(data.keys())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f809db5-c6c7-4c3b-af26-39115e65e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mol_latents, spec_latents, smile_preds, logit_scale, ids = model(data, max_charge, num_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d1258-3348-48f7-817b-c864fd2d44f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90cc23a1-f2aa-4fa3-86d4-2041bf8332cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import CombinedLoss\n",
    "\n",
    "# vocab = pickle.load(open( config['data']['vocab_path'], 'rb'))\n",
    "# class CombinedLoss(nn.Module):\n",
    "#     # under construction \n",
    "#     def __init__(self, temperature=1, threshold=0.8):\n",
    "#         super().__init__()\n",
    "#         self.temperature = temperature\n",
    "#         self.threshold = threshold\n",
    "    \n",
    "#     def forward(self, mol_features, spectra_features, logit_scale, smile_ypred, data):\n",
    "#         # spectra = spectra.squeeze(1)\n",
    "#         # spectra = spectra.squeeze(1)\n",
    "        \n",
    "#         logits = logit_scale *  mol_features @ spectra_features.t() \n",
    "        \n",
    "#         mol_sims = mol_features @ mol_features.t()\n",
    "#         spectra_sims = spectra_features @ spectra_features.t()\n",
    "#         # og_spectra_sims = spectra @ spectra.t()\n",
    "        \n",
    "#         # targets = get_spec_mat(spectra, threshold=self.threshold)\n",
    "#         targets = torch.diag(torch.ones(spectra_features.shape[0])).to(device)\n",
    "      \n",
    "#         clip_loss = (F.cross_entropy(logits, targets) + \n",
    "#                      F.cross_entropy(logits.t(), targets.t())\n",
    "#                      ) / 2\n",
    "        \n",
    "#         smile_y = data['decoder_tgt'].to(device)\n",
    "#         smile_yprob = F.log_softmax(smile_ypred, dim=2)\n",
    "        \n",
    "#         reconstruction_loss = F.nll_loss(smile_yprob.view(-1, len(vocab)),\n",
    "#                                         smile_y.view(-1))\n",
    "#         total_loss = clip_loss + reconstruction_loss\n",
    "        \n",
    "#         return total_loss, clip_loss, reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f966a3c-5a43-46e4-a8ad-86ba7a29fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = CombinedLoss().to(device)\n",
    "\n",
    "# total_loss, clip_loss, reconstruction_loss = loss_fn(mol_latents, spec_latents, logit_scale, smile_preds, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a906dc64-5f55-4b00-ae65-0e6e06717034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import train_one_epoch, validate\n",
    "# def train_one_epoch(model, dataloader, epoch, optimizer, loss_fn, focus=\"clip_loss\"):\n",
    "    \n",
    "#     running_loss = []\n",
    "#     model.to(device)\n",
    "#     model.train()\n",
    "    \n",
    "#     for i, data in enumerate(dataloader):\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         mol_latents, spec_latents, smile_preds, logit_scale, ids = model(data, max_charge, num_species)\n",
    "#         total_loss, clip_loss, reconstruction_loss = loss_fn(mol_latents, spec_latents, logit_scale, smile_preds, data)\n",
    "        \n",
    "#         if focus == \"total_loss\":\n",
    "#             total_loss.backward()\n",
    "#         elif focus == \"clip_loss\":\n",
    "#             clip_loss.backward()\n",
    "#         elif focus == \"reconstruction_loss\":\n",
    "#             reconstruction_loss.backward()\n",
    "            \n",
    "#         optimizer.step()\n",
    "        \n",
    "#         print( 'Training Epoch: {} | iteration: {}/{} | Loss: {}'.format(epoch, i, len(dataloader), total_loss.item() ), end='\\r')\n",
    "#         running_loss.append([total_loss.item(), clip_loss.item(), reconstruction_loss.item()])\n",
    "    \n",
    "#     running_loss = np.array(running_loss)\n",
    "#     return np.mean(running_loss, axis= 0)\n",
    "        \n",
    "# def validate(model, dataloader, epoch, optimizer, loss_fn):\n",
    "    \n",
    "#     running_loss = []\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     for i, data in enumerate(dataloader):    \n",
    "#         with torch.no_grad():\n",
    "#             mol_latents, spec_latents, smile_preds, logit_scale, ids = model(data, max_charge, num_species)\n",
    "#             total_loss, clip_loss, reconstruction_loss = loss_fn(mol_latents, spec_latents, logit_scale, smile_preds, data)\n",
    "    \n",
    "#         print( 'Validation Epoch: {} | iteration: {}/{} | Loss: {}'.format(epoch, i, len(dataloader), total_loss.item() ), end='\\r')\n",
    "#         running_loss.append([total_loss.item(), clip_loss.item(), reconstruction_loss.item()])\n",
    "        \n",
    "#     running_loss = np.array(running_loss)\n",
    "#     return np.mean(running_loss, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc51d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89cc1974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import save_model, load_model\n",
    "# import os\n",
    "# import yaml\n",
    "\n",
    "# def save_model(model, config, logs, name):\n",
    "    \n",
    "#     path_dir = config['train']['checkpoint_dir']          \n",
    "#     if not os.path.exists(path_dir):\n",
    "#         os.mkdir(path_dir)\n",
    "#     model_path = path_dir + '/' + name + '.pth'\n",
    "#     config_path = path_dir + '/config.yaml'\n",
    "#     logs_path = path_dir + '/logs.pickle'\n",
    "    \n",
    "#     torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "#     with open(config_path,'w') as yaml_file:\n",
    "#         yaml.dump(dict(config), yaml_file)\n",
    "#     with open(logs_path, 'wb') as file:\n",
    "#         pickle.dump(logs, file)\n",
    "        \n",
    "#     print(\"Saved to {}\".format(path_dir))\n",
    "    \n",
    "# def load_model(path_to_dir):\n",
    "#     files = os.listdir(path_to_dir)\n",
    "#     for file in files:\n",
    "#         if '.pth' in file:      \n",
    "#             model_path = path_to_dir + '/' + file\n",
    "#         if '.yaml' in file:\n",
    "#             config_path = path_to_dir + '/' + file\n",
    "#     with open(config_path,'r') as f:\n",
    "#         config = yaml.full_load(f)\n",
    "        \n",
    "#     model = CLIP(config)\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b4c3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import update_logs_and_checkpoints\n",
    "# def update_logs_and_checkpoints(model, tl, vl, epoch, logs):\n",
    "#     logs['train_total_loss'].append(tl[0])\n",
    "#     logs['train_clip_loss'].append(tl[1])\n",
    "#     logs['train_recon_loss'].append(tl[2])\n",
    "    \n",
    "#     logs['val_total_loss'].append(vl[0])\n",
    "#     logs['val_clip_loss'].append(vl[1])\n",
    "#     logs['val_recon_loss'].append(vl[2])\n",
    "    \n",
    "#     if vl[0] < logs['best_total_loss']:\n",
    "#         logs['best_total_loss'] = vl[0]\n",
    "#         logs['best_epoch'] = epoch\n",
    "#         save_model(model, config, logs, 'best_total')\n",
    "\n",
    "#     if vl[1] < logs['best_clip_loss']:\n",
    "#         logs['best_clip_loss'] = vl[1]\n",
    "#         logs['best_clip_epoch'] = epoch \n",
    "#         save_model(model, config, logs, 'best_clip')\n",
    "           \n",
    "#     if vl[2] < logs['best_recon_loss']:\n",
    "#         logs['best_recon_loss'] = vl[2]\n",
    "#         logs['best_recon_epoch'] = epoch\n",
    "#         save_model(model, config, logs, 'best_recon')\n",
    "                \n",
    "#     return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "696a5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import print_status\n",
    "# def print_status(logs, time=None):\n",
    "#     train_total_loss = logs['train_total_loss'][-1]\n",
    "#     val_total_loss = logs['train_total_loss'][-1]\n",
    "#     print(\"Latest Train_Loss: {}, Latest Val_Loss: {}\".format( train_total_loss, val_total_loss))\n",
    "#     print(\"Best Test_Loss: {}, Best Epoch: {}\".format( logs['best_total_loss'],logs['best_epoch']))\n",
    "#     print(\"=============== Time: {}========================\".format(time))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cc17ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import time\n",
    "# max_charge, num_species = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b767a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import train_clip, train_total, train_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "692e8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_clip(model, dataloaders, optimizer, loss_fn, logs, num_epochs=50 ):\n",
    "#     for epoch in range(num_epochs):\n",
    "#         start = time.time()\n",
    "#         tl = train_one_epoch(model, dataloaders['train'], epoch, optimizer, loss_fn , focus=\"clip_loss\")\n",
    "#         vl = validate(model, dataloaders['val'], epoch, optimizer, loss_fn )\n",
    "#         logs = update_logs_and_checkpoints(model, tl, vl, epoch, logs)\n",
    "#         end = time.time()\n",
    "        \n",
    "#         wandb.log(\n",
    "#             {\n",
    "#                 'epoch': epoch,\n",
    "#                 'train_total_loss':tl[0],\n",
    "#                 'train_clip_loss':tl[1],\n",
    "#                 'train_recon_loss':tl[2],    \n",
    "#                 'val_total_loss':vl[0],\n",
    "#                 'val_clip_loss':vl[1],\n",
    "#                 'val_recon_loss':vl[2],\n",
    "#             },\n",
    "#             step = epoch\n",
    "#         )\n",
    "#         clip_performance(model, dataloaders, epoch)\n",
    "#         print_status(logs, end-start)\n",
    "#     return logs\n",
    "\n",
    "# def train_recon(model, dataloaders, optimizer, loss_fn, logs, num_epochs=50 ):\n",
    "#     for epoch in range(num_epochs):\n",
    "#         start = time.time()\n",
    "#         tl = train_one_epoch(model, dataloaders['train'], epoch, optimizer, loss_fn , focus=\"reconstruction_loss\")\n",
    "#         vl = validate(model, dataloaders['val'], epoch, optimizer, loss_fn )\n",
    "#         logs = update_logs_and_checkpoints(model, tl, vl, epoch, logs)\n",
    "#         end = time.time()\n",
    "        \n",
    "#         wandb.log(\n",
    "#             {\n",
    "#                 'epoch': epoch,\n",
    "#                 'train_total_loss':tl[0],\n",
    "#                 'train_clip_loss':tl[1],\n",
    "#                 'train_recon_loss':tl[2],    \n",
    "#                 'val_total_loss':vl[0],\n",
    "#                 'val_clip_loss':vl[1],\n",
    "#                 'val_recon_loss':vl[2],\n",
    "#             },\n",
    "#             step = epoch\n",
    "#         )\n",
    "#         print_status(logs, end-start)\n",
    "#     return logs\n",
    "\n",
    "# def train_total(model, dataloaders, optimizer, loss_fn, logs, num_epochs=50 ):\n",
    "#     for epoch in range(num_epochs):\n",
    "#         start = time.time()\n",
    "#         tl = train_one_epoch(model, dataloaders['train'], epoch, optimizer, loss_fn , focus=\"total_loss\")\n",
    "#         vl = validate(model, dataloaders['val'], epoch, optimizer, loss_fn )\n",
    "#         logs = update_logs_and_checkpoints(model, tl, vl, epoch, logs)\n",
    "#         end = time.time()\n",
    "        \n",
    "#         wandb.log(\n",
    "#             {\n",
    "#                 'epoch': epoch,\n",
    "#                 'train_total_loss':tl[0],\n",
    "#                 'train_clip_loss':tl[1],\n",
    "#                 'train_recon_loss':tl[2],    \n",
    "#                 'val_total_loss':vl[0],\n",
    "#                 'val_clip_loss':vl[1],\n",
    "#                 'val_recon_loss':vl[2],\n",
    "#             },\n",
    "#             step = epoch\n",
    "#         )\n",
    "#         print_status(logs, end-start)\n",
    "#     return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6697478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CLIP(config)\n",
    "# model.to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), config['train']['lr'] )\n",
    "# loss_fn = CombinedLoss().to(device)\n",
    "\n",
    "# logs = config['train']['logs']\n",
    "\n",
    "# global max_charge, num_species\n",
    "\n",
    "# dataloaders, max_charge, num_species = prepare_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b85e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.rand(10,512)\n",
    "# b = torch.rand(10,512)\n",
    "# torch.cat([a,b], 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db8e74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import distance_distribution, top_scores, clip_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd52a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def top_scores(mat1, mat2, offset):\n",
    "#     \"\"\"\n",
    "#     mat1 is [testset]\n",
    "#     mat2 is  [testset + trainset]\n",
    "    \n",
    "#     \"\"\"\n",
    "#     hits = []\n",
    "#     tops = [1,2,3,4,5,10]\n",
    "#     score = [0] * (len(tops))\n",
    "\n",
    "#     sims = mat1 @ mat2.t() # sims shape is [test_size, test+train_size]\n",
    "#     for k in range(len(tops)):\n",
    "#         for i, row in enumerate(sims):\n",
    "#             max_sims, ids = torch.topk(row, tops[k])\n",
    "#             if i in ids:\n",
    "#                 score[k] += 1\n",
    "#         score[k] = score[k] / len(row)\n",
    "#         # break\n",
    "#     return np.array(tops), np.array(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0251b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def distance_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e9e7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %matplotlib inline\n",
    "# def distance_distribution(molmat, specmat):\n",
    "#     sims = molmat @ specmat.t()\n",
    "#     diagonals = torch.diagonal(sims, 0).detach().numpy()\n",
    "#     sims = np.random.choice(sims.view(-1).detach().numpy(), len(diagonals))\n",
    "#     vals = np.concatenate((sims, diagonals), axis=0)\n",
    "#     pairs = [\"pairs\"] * len(diagonals)\n",
    "#     nonpairs = [\"others\"] * len(sims)\n",
    "#     df = pd.DataFrame()\n",
    "#     df['distance'] = vals\n",
    "#     df['labels'] = pairs + nonpairs\n",
    "#     sns.histplot(df, x='distance', hue='labels', kde=True, bins=50)\n",
    "     \n",
    "#     return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d400375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance_distribution(torch.rand(100,128), torch.rand(100,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec01554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# def clip_performance(model, dataloaders, epoch):\n",
    "#     model.eval()\n",
    "#     molembeds = []\n",
    "#     specembeds = []\n",
    "#     for i, data in tqdm(enumerate(dataloaders['test'])):    \n",
    "#         with torch.no_grad():\n",
    "#             mol_latents, spec_latents, smile_preds, logit_scale, ids = model(data, max_charge, num_species)\n",
    "#             molembeds.append(mol_latents)\n",
    "#             specembeds.append(spec_latents)\n",
    "#     test_molembeds = torch.cat(molembeds, 0)\n",
    "#     test_specembeds = torch.cat(specembeds, 0)\n",
    "    \n",
    "#     molembeds = []\n",
    "#     specembeds = []\n",
    "#     for i, data in tqdm(enumerate(dataloaders['train'])):    \n",
    "#         with torch.no_grad():\n",
    "#             mol_latents, spec_latents, smile_preds, logit_scale, ids = model(data, max_charge, num_species)\n",
    "#             molembeds.append(mol_latents)\n",
    "#             specembeds.append(spec_latents)\n",
    "#     train_molembeds = torch.cat(molembeds, 0)\n",
    "#     train_specembeds = torch.cat(specembeds, 0)\n",
    "    \n",
    "#     all_molembeds = torch.cat(( test_molembeds, train_molembeds), axis = 0)\n",
    "    \n",
    "#     all_molembeds = all_molembeds.to(\"cpu\")\n",
    "#     train_molembeds = train_molembeds.to(\"cpu\")\n",
    "#     train_specembeds = train_specembeds.to(\"cpu\")\n",
    "#     test_molembeds = test_molembeds.to(\"cpu\")\n",
    "#     test_specembeds = test_specembeds.to(\"cpu\")\n",
    "    \n",
    "#     tops, scores, hits = top_scores(test_specembeds, all_molembeds)\n",
    "#     for k, acc in zip(tops, scores):\n",
    "#         # print(\"Full Top {} Spec\".format(k), acc)\n",
    "#         wandb.log({\"Full Top {} Spec\".format(k): acc}, step=epoch)\n",
    "    \n",
    "#     tops, scores, hits = top_scores(test_specembeds, test_molembeds )\n",
    "#     for k, acc in zip(tops, scores):\n",
    "#         # print(\"Test Top {} Spec\".format(k), acc)\n",
    "#         wandb.log({\"Test Top {} Spec\".format(k): acc}, step=epoch)\n",
    "\n",
    "#     wandb.log({'Distance Distribution Train': distance_distribution(train_molembeds, train_specembeds)}, step=epoch) \n",
    "#     wandb.log({'Distance Distribution Test': distance_distribution(test_molembeds, test_specembeds)}, step=epoch) \n",
    "\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffd00830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_performance(model, dataloaders, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfe7a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2218aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logs = {}\n",
    "\n",
    "def run(config):\n",
    "    with wandb.init(project= config['wandb']['project_name'],\n",
    "                    dir= config['wandb']['dir'],\n",
    "                    name=config['wandb']['run_name'] ,\n",
    "                    config = config,\n",
    "                    job_type= config['wandb']['job_type'],\n",
    "                    save_code= True):\n",
    "        config = wandb.config\n",
    "        global logs, max_charge, num_species\n",
    "        model = CLIP(config)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), config['train']['lr'] )\n",
    "        vocab = pickle.load(open(config['data']['vocab_path'], 'rb'))\n",
    "        loss_fn = CombinedLoss(vocab).to(device)\n",
    "        \n",
    "        logs = config['train']['logs']\n",
    "        \n",
    "        dataloaders, max_charge, num_species = prepare_data(config)\n",
    "        \n",
    "        config['data']['max_charge'] = max_charge\n",
    "        config['data']['num_species'] = num_species\n",
    "        \n",
    "        print(\"Starting Training\")\n",
    "        \n",
    "        wandb.watch(model, loss_fn, log='all', log_freq=100, log_graph=True)\n",
    "        train_clip(config, model, dataloaders, optimizer, loss_fn, logs, 100)\n",
    "        train_recon(config, model, dataloaders, optimizer, loss_fn, logs,  50)\n",
    "        train_total(config, model, dataloaders, optimizer, loss_fn, logs, 50)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df4c23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['data']['batch_size'] = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab9873",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9ed21bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c1e8a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d54530f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bedd0793db654eeab05e6cbf4498f450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273ed4cc18de4faa80730d4d03a59d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloaders, max_charge, num_species = prepare_data(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ce46263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9), 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_charge, num_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d55bba7-e2aa-484d-9c6a-f8f9b15ab59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CLIP(config)\n",
    "# model.to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0001)\n",
    "# loss_fn = CombinedLoss().to(device)\n",
    "\n",
    "\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# for i in range(5):\n",
    "#     train_losses.append(train_one_epoch(model, dataloaders['train'], i, optimizer, loss_fn , focus=\"clip_loss\"))\n",
    "#     val_losses.append(validate(model, dataloaders['test'], i, optimizer, loss_fn))\n",
    "#     # print(\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162e537-d54e-4f56-bd97-044fb381d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# plt.plot(train_losses[:, 0], label=\"total_loss\")\n",
    "# plt.plot(val_losses[:, 0],label=\"val_losses\")\n",
    "# plt.title(\"\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b600f40-7c85-4c24-a08b-68c60388445d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad25edf0-2146-4f5b-98c5-6506b7f9dbfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbdd-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
