{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['data'] = {\"qm9_broad_ir_path\":'/home2/kanakala.ganesh/ir_data/qm9_broad_ir.pkl',\n",
    "                  \"vocab_path\":'/home2/kanakala.ganesh/CLIP_PART_1/data/qm9_vocab.pkl',\n",
    "                  \"datafiles\" : {\n",
    "                        'train': '/home2/kanakala.ganesh/ir_data/raw_train.pickle',\n",
    "                        'test':  '/home2/kanakala.ganesh/ir_data/raw_test.pickle',\n",
    "                        'val':   '/home2/kanakala.ganesh/ir_data/raw_val.pickle'\n",
    "                        },\n",
    "                  \"normalization\" : \"unit\",\n",
    "                  \"shuffle\": True,\n",
    "                  \"batch_size\":200,\n",
    "                  \"seq_len\":70,\n",
    "                  \"splits\":[0.8, 0.1, 0.1],\n",
    "                  \"num_workers\":20\n",
    "                }\n",
    "\n",
    "config['molecule_encoder'] = {\n",
    "    'attention': 1,\n",
    "    'coords_weight' :1.0,\n",
    "    'device': \"cuda\",\n",
    "    'hidden_nf':256,\n",
    "    'in_edge_nf':0,\n",
    "    'in_node_nf':15,\n",
    "    'n_layers': 3,\n",
    "    'node_attr': 1,\n",
    "    'output_size':512\n",
    "}\n",
    "\n",
    "config['molecule_decoder'] = {\n",
    "    'in_size': 512,\n",
    "    'latent_size' : 512,\n",
    "    'hidden_size': 512,\n",
    "    'n_layers' : 5,\n",
    "    'n_heads' : 4\n",
    "}\n",
    "\n",
    "config['spectra_encoder'] = {\n",
    "    'd_ff': 1024,\n",
    "    'dropout': 0.0,\n",
    "    'dropout_emb': 0.1,\n",
    "    'h_dim': 256,\n",
    "    'max_time_steps': 1000,\n",
    "    'num_heads': 7,\n",
    "    'num_layers': 5,\n",
    "    'output_size': 512,\n",
    "    'patch_size': 7,\n",
    "    'use_clf_token': True,\n",
    "}\n",
    "\n",
    "config['train'] = {\n",
    "    'lr':0.0001,\n",
    "    'temperature' :0.1,\n",
    "    'checkpoint_dir': \"checkpoints/temp\",\n",
    "    'device':\"cuda\",\n",
    "    'num_epochs':100,\n",
    "    'threshold': 0.9999,\n",
    "    'weight_decay': 1.0e-06\n",
    "}\n",
    "\n",
    "config['wandb'] = {\n",
    "    \"dir\": \"/scratch/kanakala.ganesh/\",\n",
    "    \"job_type\": \"sample\",\n",
    "    \"project_name\": \"CLIP_Full_testing\",\n",
    "    \"run_name\": \"RUN_testing\"\n",
    "}\n",
    "config['data']['max_charge'] = None\n",
    "config['data']['num_species'] = None\n",
    "\n",
    "config['train']['logs'] = {\n",
    "            'train_total_loss':[],\n",
    "            'train_clip_loss':[],\n",
    "            'train_recon_loss':[],\n",
    "            \n",
    "            'val_total_loss':[],\n",
    "            'val_clip_loss':[],\n",
    "            'val_recon_loss':[],\n",
    "            \n",
    "            'test_total_loss':[],\n",
    "            'test_clip_loss':[],\n",
    "            'test_recon_loss':[],\n",
    "            \n",
    "            'best_epoch': -1,\n",
    "            'best_clip_epoch': -1,\n",
    "            'best_recon_epoch':-1,\n",
    "            \n",
    "            'best_total_loss':1000,\n",
    "            'best_clip_loss':1000,\n",
    "            'best_recon_loss':1000\n",
    "        }\n",
    "from PrepareData import prepare_data\n",
    "import torch\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.nn import functional as F\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import wandb\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "\n",
    "from train_utils import CombinedLoss\n",
    "from train_utils import train_clip, train_total, train_recon\n",
    "\n",
    "\n",
    "logs, max_charge, num_species = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pickle\n",
    "from qm9 import utils as qm9_utils\n",
    "from models.vit import ViT\n",
    "from qm9.models import EGNN\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "dtype = torch.float32\n",
    "\n",
    "from models.decoder import LatentToMol\n",
    "\n",
    "def set_up_causal_mask(seq_len):\n",
    "    mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    mask.requires_grad = False\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pickle\n",
    "from qm9 import utils as qm9_utils\n",
    "from models.vit import ViT\n",
    "from qm9.models import EGNN\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "dtype = torch.float32\n",
    "\n",
    "from models.decoder import LatentToMol\n",
    "\n",
    "def set_up_causal_mask(seq_len):\n",
    "    mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    mask.requires_grad = False\n",
    "    return mask\n",
    "\n",
    "class PositionalEncodings(nn.Module):\n",
    "    \"\"\"Attention is All You Need positional encoding layer\"\"\"\n",
    "\n",
    "    def __init__(self, seq_len, d_model, p_dropout):\n",
    "        \"\"\"Initializes the layer.\"\"\"\n",
    "        super(PositionalEncodings, self).__init__()\n",
    "        token_positions = torch.arange(start=0, end=seq_len).view(-1, 1)\n",
    "        dim_positions = torch.arange(start=0, end=d_model).view(1, -1)\n",
    "        angles = token_positions / (10000 ** ((2 * dim_positions) / d_model))\n",
    "\n",
    "        encodings = torch.zeros(1, seq_len, d_model)\n",
    "        encodings[0, :, ::2] = torch.cos(angles[:, ::2])\n",
    "        encodings[0, :, 1::2] = torch.sin(angles[:, 1::2])\n",
    "        encodings.requires_grad = False\n",
    "        self.register_buffer(\"positional_encodings\", encodings)\n",
    "\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs forward pass of the module.\"\"\"\n",
    "        x = x + self.positional_encodings\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class bottle(nn.Module):\n",
    "    def __init__(self, seq_len, hidden_size):\n",
    "        super(bottle, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Linear(seq_len*hidden_size, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, hidden_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        inp = inp.view(-1, self.seq_len * self.hidden_size)\n",
    "        embed = self.down(inp)\n",
    "        # out = self.up(embed)\n",
    "        return embed\n",
    "       \n",
    "\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vocab = pickle.load(open(config['data']['vocab_path'], 'rb'))\n",
    "        self.temperature = config['train']['temperature']\n",
    "        self.max_charge = config['data']['max_charge']\n",
    "        self.num_species = config['data']['num_species']\n",
    "        self.embed = nn.Embedding(len(self.vocab), config['molecule_decoder']['hidden_size'], padding_idx=self.vocab.pad_index)\n",
    "        self.pe = PositionalEncodings(d_model=config['molecule_decoder']['hidden_size'], p_dropout=0.1, seq_len=config['data']['seq_len'])\n",
    "        \n",
    "        transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=config['molecule_decoder']['hidden_size'],\n",
    "                                                               nhead=8,\n",
    "                                                               dropout=0.1,\n",
    "                                                               batch_first=True)\n",
    "        \n",
    "        self.trfmencoder = nn.TransformerEncoder(encoder_layer=transformer_encoder_layer,\n",
    "                                                 num_layers=3)\n",
    "        \n",
    "        self.Spectra_Encoder = ViT(\n",
    "            patch_size = self.config['spectra_encoder']['patch_size'], \n",
    "            num_layers = self.config['spectra_encoder']['num_layers'], \n",
    "            h_dim = self.config['spectra_encoder']['h_dim'], \n",
    "            num_heads = self.config['spectra_encoder']['num_heads'], \n",
    "            output_size = self.config['spectra_encoder']['output_size'], \n",
    "            d_ff=self.config['spectra_encoder']['d_ff'], \n",
    "            max_time_steps=self.config['spectra_encoder']['max_time_steps'], \n",
    "            use_clf_token=self.config['spectra_encoder']['use_clf_token'],\n",
    "            dropout = self.config['spectra_encoder']['dropout'],\n",
    "            dropout_emb = self.config['spectra_encoder']['dropout_emb']   \n",
    "        )\n",
    "        \n",
    "        self.smiles_decoder = LatentToMol(\n",
    "            in_size=self.config['molecule_decoder']['latent_size'],\n",
    "            hidden_size=self.config['molecule_decoder']['hidden_size'], \n",
    "            n_layers=self.config['molecule_decoder']['n_layers'], \n",
    "            n_heads = self.config['molecule_decoder']['n_heads'],\n",
    "            seq_len=self.config['data']['seq_len'], \n",
    "            vocab = self.vocab)\n",
    "        \n",
    "        self.bottle = bottle(config['data']['seq_len'], config['molecule_decoder']['hidden_size'])\n",
    "        \n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * self.temperature)\n",
    "        \n",
    "    \n",
    "    def forward_mol(self, data):\n",
    "        smi = data['decoder_inp'].to(device)\n",
    "        smi = self.embed(smi)\n",
    "        # smi = self.res_block(smi)\n",
    "        smi = self.pe(smi)\n",
    "        mem = self.trfmencoder(smi)\n",
    "        mol_features = self.bottle(mem)\n",
    "        \n",
    "        mol_features = mol_features / mol_features.norm(dim=1, keepdim=True)\n",
    "        \n",
    "        return mol_features\n",
    "    \n",
    "    def forward_spec(self, data):\n",
    "        spectra = data['IR'].to(device, dtype)\n",
    "        spectra = torch.unsqueeze(spectra, 1)\n",
    "        spectra = torch.unsqueeze(spectra, 1)\n",
    "        \n",
    "        spectra_features = self.Spectra_Encoder(spectra)\n",
    "        spectra_features = spectra_features / spectra_features.norm(dim=1, keepdim=True)\n",
    "        \n",
    "        return spectra_features\n",
    "    \n",
    "    def forward_decoder(self, data, spec_latents):\n",
    "        smi = data['decoder_inp'].to(device)\n",
    "        tgt = data['decoder_tgt'].to(device)\n",
    "        tgt_padding_mask = data['tgt_padding_mask'].to(device)\n",
    "        tgt_mask = set_up_causal_mask(self.config['data']['seq_len']).to(device)\n",
    "        \n",
    "        pred = self.smiles_decoder(spec_latents,\n",
    "                                   smi,\n",
    "                                   tgt_mask,\n",
    "                                   tgt_padding_mask)\n",
    "        return pred\n",
    "        \n",
    "    def forward(self, data):\n",
    "        logits_scale = self.logit_scale.exp()\n",
    "        \n",
    "        mol_latents = self.forward_mol(data)\n",
    "        spec_latents = self.forward_spec(data)\n",
    "        \n",
    "        smile_preds = self.forward_decoder(data, spec_latents)\n",
    "        \n",
    "        return mol_latents, spec_latents, smile_preds, logits_scale, data['index'] \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkganeshchandan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /scratch/kanakala.ganesh/wandb/ wasn't writable, using system temp directory.\n",
      "wandb: WARNING Path /scratch/kanakala.ganesh/wandb/ wasn't writable, using system temp directory\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/run-20230723_031125-26k6g2d1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kganeshchandan/CLIP_Full_testing/runs/26k6g2d1\" target=\"_blank\">RUN_testing</a></strong> to <a href=\"https://wandb.ai/kganeshchandan/CLIP_Full_testing\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of GPUs available 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49720c334bb4bb69865ef073d344fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bfc86c33024d5a84f97bb9256e47d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of batches  250\n",
      "no of batches  30\n",
      "no of batches  25\n",
      "Starting Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to checkpoints/tempration: 24/25 | Loss: 8.679645538330078\n",
      "Saved to checkpoints/temp\n",
      "Saved to checkpoints/temp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "2it [00:00,  2.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>train_clip_loss</td><td>▁</td></tr><tr><td>train_recon_loss</td><td>▁</td></tr><tr><td>train_total_loss</td><td>▁</td></tr><tr><td>val_clip_loss</td><td>▁</td></tr><tr><td>val_recon_loss</td><td>▁</td></tr><tr><td>val_total_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_clip_loss</td><td>5.93478</td></tr><tr><td>train_recon_loss</td><td>3.08123</td></tr><tr><td>train_total_loss</td><td>9.01601</td></tr><tr><td>val_clip_loss</td><td>5.62017</td></tr><tr><td>val_recon_loss</td><td>3.06288</td></tr><tr><td>val_total_loss</td><td>8.68305</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">RUN_testing</strong>: <a href=\"https://wandb.ai/kganeshchandan/CLIP_Full_testing/runs/26k6g2d1\" target=\"_blank\">https://wandb.ai/kganeshchandan/CLIP_Full_testing/runs/26k6g2d1</a><br/>Synced 7 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/tmp/wandb/run-20230723_031125-26k6g2d1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1212, in _call_impl\n    result = forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_23124/2753486839.py\", line 145, in forward\n    spec_latents = self.forward_spec(data)\n  File \"/tmp/ipykernel_23124/2753486839.py\", line 124, in forward_spec\n    spectra_features = self.Spectra_Encoder(spectra)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home2/kanakala.ganesh/CLIP_PART_1/models/vit.py\", line 124, in forward\n    x = self.enc(x)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home2/kanakala.ganesh/CLIP_PART_1/models/vit.py\", line 92, in forward\n    x = layer(x, mask=mask)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home2/kanakala.ganesh/CLIP_PART_1/models/vit.py\", line 56, in forward\n    x = self.ffn(x_) + x\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n    input = module(input)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n    return F.linear(input, self.weight, self.bias)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.70 GiB total capacity; 21.64 GiB already allocated; 33.69 MiB free; 22.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m         train_clip(config, model, dataloaders, optimizer, loss_fn, logs, \u001b[39m0\u001b[39m, \u001b[39m200\u001b[39m)\n\u001b[1;32m     36\u001b[0m         \u001b[39m# train_recon(config, model, dataloaders, optimizer, loss_fn, logs, 200, 300)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[39m# train_total(config, model, dataloaders, optimizer, loss_fn, logs, 300,400)\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m run(config)\n",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting Training\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m wandb\u001b[39m.\u001b[39mwatch(model, loss_fn, log\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m, log_freq\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, log_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m train_clip(config, model, dataloaders, optimizer, loss_fn, logs, \u001b[39m0\u001b[39;49m, \u001b[39m200\u001b[39;49m)\n",
      "File \u001b[0;32m~/CLIP_PART_1/train_utils.py:211\u001b[0m, in \u001b[0;36mtrain_clip\u001b[0;34m(config, model, dataloaders, optimizer, loss_fn, logs, start, num_epochs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     wandb\u001b[39m.\u001b[39mlog(\n\u001b[1;32m    199\u001b[0m         {\n\u001b[1;32m    200\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m: epoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         step \u001b[39m=\u001b[39m epoch\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    210\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 211\u001b[0m         clip_performance(config, model, dataloaders, epoch)\n\u001b[1;32m    212\u001b[0m     print_status(logs, end\u001b[39m-\u001b[39mstart)\n\u001b[1;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m logs\n",
      "File \u001b[0;32m~/CLIP_PART_1/train_utils.py:322\u001b[0m, in \u001b[0;36mclip_performance\u001b[0;34m(config, model, dataloaders, epoch)\u001b[0m\n\u001b[1;32m    319\u001b[0m specembeds \u001b[39m=\u001b[39m []\n\u001b[1;32m    320\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(dataloaders[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m])):    \n\u001b[0;32m--> 322\u001b[0m     mol_latents, spec_latents, smile_preds, logit_scale, ids \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m    323\u001b[0m     molembeds\u001b[39m.\u001b[39mappend(mol_latents)\n\u001b[1;32m    324\u001b[0m     specembeds\u001b[39m.\u001b[39mappend(spec_latents)\n",
      "File \u001b[0;32m~/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[0;32m~/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[39m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         output\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m     90\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1212, in _call_impl\n    result = forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_23124/2753486839.py\", line 145, in forward\n    spec_latents = self.forward_spec(data)\n  File \"/tmp/ipykernel_23124/2753486839.py\", line 124, in forward_spec\n    spectra_features = self.Spectra_Encoder(spectra)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home2/kanakala.ganesh/CLIP_PART_1/models/vit.py\", line 124, in forward\n    x = self.enc(x)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home2/kanakala.ganesh/CLIP_PART_1/models/vit.py\", line 92, in forward\n    x = layer(x, mask=mask)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home2/kanakala.ganesh/CLIP_PART_1/models/vit.py\", line 56, in forward\n    x = self.ffn(x_) + x\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n    input = module(input)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home2/kanakala.ganesh/miniconda3/envs/sbdd-env/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n    return F.linear(input, self.weight, self.bias)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.70 GiB total capacity; 21.64 GiB already allocated; 33.69 MiB free; 22.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "from train_utils import validate, train_one_epoch\n",
    "def run(config):\n",
    "    with wandb.init(project= config['wandb']['project_name'],\n",
    "                    dir= config['wandb']['dir'],\n",
    "                    name=config['wandb']['run_name'] ,\n",
    "                    config = config,\n",
    "                    job_type= config['wandb']['job_type'],\n",
    "                    save_code= True):\n",
    "        config = wandb.config\n",
    "        global logs, max_charge, num_species\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(\"No of GPUs available\", num_gpus)\n",
    "        model = CLIP(config)\n",
    "        model.to(device)\n",
    "        model = torch.nn.parallel.DataParallel(model)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                                      lr = config['train']['lr'],\n",
    "                                      weight_decay=config['train']['weight_decay'])\n",
    "        vocab = pickle.load(open(config['data']['vocab_path'], 'rb'))\n",
    "        loss_fn = CombinedLoss(vocab).to(device)\n",
    "        \n",
    "        logs = config['train']['logs']\n",
    "        \n",
    "        dataloaders, max_charge, num_species = prepare_data(config)\n",
    "        for d in dataloaders:\n",
    "            print(\"no of batches \", len(dataloaders[d]))\n",
    "        \n",
    "        config['data']['max_charge'] = max_charge\n",
    "        config['data']['num_species'] = num_species\n",
    "        \n",
    "        print(\"Starting Training\")\n",
    "        \n",
    "        wandb.watch(model, loss_fn, log='all', log_freq=100, log_graph=True)\n",
    "        train_clip(config, model, dataloaders, optimizer, loss_fn, logs, 0, 200)\n",
    "        # train_recon(config, model, dataloaders, optimizer, loss_fn, logs, 200, 300)\n",
    "        # train_total(config, model, dataloaders, optimizer, loss_fn, logs, 300,400)\n",
    "run(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbdd-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
